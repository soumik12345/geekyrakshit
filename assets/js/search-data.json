{
  
    
        "post0": {
            "title": "Efficient Graph-Based Image Segmentation",
            "content": "In this article, we would be discussing the paper Efficient Graph-Based Image Segmentation by Pedro F. Felzenszwalb from Artificial Intelligence Lab, Massachusetts Institute of Technology and Daniel P. Huttenlocher from Computer Science Department, Cornell University. . What is Segmentation? . Before delving any further, let us try to understand a bit more about the task at hand. What exactly do we mean by image segmentation? . Image Segmentation can be defined as the task of partitioning a given image into a set of disjoint regions usually with a goal of simplifying the representation of the image into something that is more meaningful and easier to analyze. It has been observed from past segmentation approaches that . It‚Äôs not adequate to assume that regions have nearly constant or slowly varying intensities. | The determination of boundary between the regions cannot only use local decision criteria. | . The Idea . Observing the performance of the past methods of image segmentation, the objective of this paper has been to develop an alogorithm for image segmentation that . captures perceptually important regions that reflect global aspect. | runs efficiently at near linear time complexity, O(n‚àólog(n))O(n * log(n))O(n‚àólog(n)) in this case. | . The idea proposed by Felzenszwalb and Huttenlocher is based on selecting edges from a graph, where each pixel corresponds to a node in the graph, and certain neighboring pixels are connected by undirected edges such that weights on each edge measure the dissimilarity between pixels. However, unlike the classical methods that predate this paper, this technique adaptively adjusts the segmentation criterion based on the degree of variability in neighboring regions of the image. This results in a method that, while making greedy decisions, can be shown to obey certain non-obvious global properties. The adaptive criteria is defined as follows: . There is a boundary between two adjacent regions Ci and Cj, i.e, Variation across Ci and Cj is greater than variation within Ci or Cj individually. Problem Formulation . Let G=(V,E)G=(V, E)G=(V,E) be an undirected graph such that, . viœµVv_{i} epsilon Vvi‚ÄãœµV: set of vertices or pixels in the image to be segmented. | e=(vi,vj)œµEe = (v_{i}, v_{j}) epsilon Ee=(vi‚Äã,vj‚Äã)œµE: set of edges corresponding to pairs of neighbouring vertices or pixels. | Each edge e=(vi,vj)œµEe = (v_{i}, v_{j}) epsilon Ee=(vi‚Äã,vj‚Äã)œµE has a weight w(vi,vj)w(v_{i}, v_{j})w(vi‚Äã,vj‚Äã) denoting the dissimilarity between vi and vj. | . SSS is a segmentation of a graph G such that G‚Ä≤=(V,E‚Ä≤){G}&amp;#x27; = (V, {E}&amp;#x27;)G‚Ä≤=(V,E‚Ä≤) where E‚Ä≤‚äÇE{E}&amp;#x27; subset EE‚Ä≤‚äÇE. SSS divides GGG into G‚Ä≤{G}&amp;#x27;G‚Ä≤ such that it contains distinct components (or regions) CCC. . Graph Representation . Let us consider an image consisting of 12 pixels with the following intensities: . 1212141412121212131316161101109090121240404545100100Viewer does not support full SVG 1.1 For representing the image as an undirected graph, we can use either the N4 system or the N8 system. According to N4 system, each node can be connected to 4 neighbouring nodes. Hence, by this system, the graph would be: . 1212141412121212131316161101109090121240404545100100Viewer does not support full SVG 1.1 According to N8 system (8 connections with neighbouring nodes), the graph would be: . 1212141412121212131316161101109090121240404545100100Viewer does not support full SVG 1.1 In this case, let us procced with the N4 Graph. Let us assign weights to the edges based upon the difference between the intensities. . 1212141412121212131316161101109090121240404545100100222200282855555511229898787811242465651010Viewer does not support full SVG 1.1 Segmentation Formulation . Thus the segmentation problem can be formulated as partition of the vertex set V of the given undirected graph G into components C1, C2, ‚Ä¶.. such that, . edges between two vertices in the same segment Ci should have lower weights . | edges between two vertices in different segments Ci and Cj should have lower weights . | . Going by this formulation, one possible one possible solution can the sets C1, C2 and C3 such that, . 12121414121212121313161611011090901212404045451001002222003394942020282855555511229898787811242465651010C1C1C3C3C2C2Viewer does not support full SVG 1.1 Partition Strategy . Internal Difference . The paper defines **Internal Difference** *(Int)* by of a componenet C ‚äÜ V as the largest weight in the *Minimum Spanning Tree* `MST(C, E)` of that component, i.e, $$Int(C) = max_{e epsilon MST(C, E)} w(e)$$ $$Int(C) = 0$$ if C has a single pixel. One intuition underlying this measure is that a given component C only remains connected when edges of weight at least Int(C) are considered. . By this definition, . Int(C1) = max(MST(C1, E)) = max(2, 2, 0, 1, 2, 1) = 2 . | Int(C2) = 5 . | Int(C3) = max(20, 10) = 20 . | . Component Difference . The paper also **Component Difference** *(Dif)* as the difference between two components C1, C2 ‚äÜ V to be the minimum weight edge connecting the two components, i.e, $$Dif(C_{1}, C_{2}) = min_{v_{i} epsilon C_{1}, v_{j} epsilon C_{2}, (v_{i}, v_{j}) epsilon E} w((v_{i}, v_{j}))$$ If there is no edge connecting C1 and C2, we let $$Dif(C_{1}, C_{2}) = infty$$ . This measure of difference could in principle be problematic, because it reflects only the smallest edge weight between two components. In practice we have found that the measure works quite well in spite of this apparent limitation. Moreover, changing the definition to use the median weight, or some other quantile, in order to make it more robust to outliers, makes the problem of finding a good segmentation NP-hard. Thus a small change to the segmentation criterion vastly changes the difficulty of the problem. . By this definition, . Dif(C1, C2) = 24 . | Dif(C1, C3) = 94 . | Dif(C2, C3) = 55 . | . Boundary Predicate . The criterion for evaluating the evidence of a boundary between a pair of adjacent components is that, . There exists a boundary between two components if the Componenet Difference between the components is greater than the Internal Differences of either of the components D(C1,C2)={trueDif(Ci,Cj)&gt;min(Int(Ci),Int(Cj))falseotherwise}D(C_{1}, C_{2}) = begin{Bmatrix} true &amp; Dif(C_{i}, C_{j}) &gt; min(Int(C_{i}), Int(C_{j})) false &amp; otherwise end{Bmatrix}D(C1‚Äã,C2‚Äã)={truefalse‚ÄãDif(Ci‚Äã,Cj‚Äã)&gt;min(Int(Ci‚Äã),Int(Cj‚Äã))otherwise‚Äã} . However, this predicate is not a good example of local property because it makes the algorithm predict a lot of small components with small size, in the extreme case if Internal Difference is 0, then the component becomes a single pixel. . In order the counter this effect, the paper introduces a Threshold Function œÑ that controls the degree to which the difference between two components must be greater than their internal differences in order for there to be evidence of a boundary between them. It is given by œÑ(c)=k‚à£C‚à£ tau(c) = frac{k}{|C|}œÑ(c)=‚à£C‚à£k‚Äã, where |C| denotes the size of C, and k is some constant parameter. This means, for small components we require stronger evidence for a boundary. In practice k sets a scale of observation, in that a larger k causes a preference for larger components. Note, however, that k is not a minimum component size. Smaller components are allowed when there is a sufficiently large difference between neighboring components. . The original predicate is thus rewritten as D(C1,C2)={trueDif(Ci,Cj)&gt;min(Int(Ci)+œÑ(Ci),Int(Cj)+œÑ(Ci))falseotherwise}D(C_{1}, C_{2}) = begin{Bmatrix} true &amp; Dif(C_{i}, C_{j}) &gt; min(Int(C_{i}) + tau(C_{i}), Int(C_{j}) + tau(C_{i})) false &amp; otherwise end{Bmatrix}D(C1‚Äã,C2‚Äã)={truefalse‚ÄãDif(Ci‚Äã,Cj‚Äã)&gt;min(Int(Ci‚Äã)+œÑ(Ci‚Äã),Int(Cj‚Äã)+œÑ(Ci‚Äã))otherwise‚Äã}. . Segmentation Algorithm . Input: . A graph G = (V, E) with n vertices and m edges. | A constant parameter k. | . Output: . A partition of V into segments S=(C1, C1, ...) | . Initialization: . Consider each vertex a single element componenet. | Initialize each component Ci with Int(Ci) = 0. | Sort all edges e ‚àà E into (e1, e2, ..., em) according to their weights in a non-decreasing order. | . Iteration Step q=(1, m): . Take step eq = (vi, vj), where vi = Ci and vj = Cj. . | If Cj != Cj . If boundary predicate D(Cj, Cj) = false, merge all the componenets Ci and Cj. . | If Ci and Cj are merged, Int(Ci ‚à™ Cj) = w(eq). . | . | q = q + 1 . | . The Merge Condition in the iteration step is defined as . D(Ci,Cj)=falseD(C_{i}, C_{j}) = falseD(Ci‚Äã,Cj‚Äã)=false . This happens if Dif(Ci,Cj)‚â§min(Int(Ci)+œÑ(Ci),Int(Cj)+œÑ(Ci))Dif(C_{i}, C_{j}) leq min(Int(C_{i}) + tau(C_{i}), Int(C_{j}) + tau(C_{i}))Dif(Ci‚Äã,Cj‚Äã)‚â§min(Int(Ci‚Äã)+œÑ(Ci‚Äã),Int(Cj‚Äã)+œÑ(Ci‚Äã)). . This means that Dif(Ci,Cj)‚â§Int(Ci)+kœÑ(Ci)Dif(C_{i}, C_{j}) leq Int(C_{i}) + frac{k}{ tau(C_{i})}Dif(Ci‚Äã,Cj‚Äã)‚â§Int(Ci‚Äã)+œÑ(Ci‚Äã)k‚Äã or Dif(Ci,Cj)‚â§Int(Cj)+kœÑ(Cj)Dif(C_{i}, C_{j}) leq Int(C_{j}) + frac{k}{ tau(C_{j})}Dif(Ci‚Äã,Cj‚Äã)‚â§Int(Cj‚Äã)+œÑ(Cj‚Äã)k‚Äã. . We can also write the condition as w(eq)‚â§Int(Ci)+kœÑ(Ci)w(e_{q}) leq Int(C_{i}) + frac{k}{ tau(C_{i})}w(eq‚Äã)‚â§Int(Ci‚Äã)+œÑ(Ci‚Äã)k‚Äã or w(eq)‚â§Int(Cj)+kœÑ(Cj)w(e_{q}) leq Int(C_{j}) + frac{k}{ tau(C_{j})}w(eq‚Äã)‚â§Int(Cj‚Äã)+œÑ(Cj‚Äã)k‚Äã. . Implementation . I created a minimal python implementation of Felzenszwalb Segmentation. It can be installed using pip install felzenszwalb-segmentation. . import numpy as np from glob import glob from PIL import Image from matplotlib import pyplot as plt from felzenszwalb_segmentation import segment image_files = glob(&#39;./VOCdevkit/VOC2012/JPEGImages/*.jpg&#39;) image = np.array(Image.open(image_files[10])) segmented_image = segment(image, 0.2, 400, 50) fig = plt.figure(figsize=(12, 12)) a = fig.add_subplot(1, 2, 1) plt.imshow(image) a = fig.add_subplot(1, 2, 2) plt.imshow(segmented_image.astype(np.uint8)) plt.show() . Results . . . . .",
            "url": "https://soumik12345.github.io/geekyrakshit/algebra/computervision/convolution/maths/python/2020/09/17/efficient-graph-based-image-segmentation.html",
            "relUrl": "/algebra/computervision/convolution/maths/python/2020/09/17/efficient-graph-based-image-segmentation.html",
            "date": " ‚Ä¢ Sep 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://soumik12345.github.io/geekyrakshit/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://soumik12345.github.io/geekyrakshit/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Depthwise Separable Convolutions in Deep Learning",
            "content": "The Convolution operation is a widely used function in Functional Analysis, Image Processing Deep Learning. The convolution operation when applied on two functions f and g, produces a third function expressing how the shape of one is modified by the other. While it is immensely popular, especially in the domain of Deep Learning, the vanilla convolution operation is quite expensive computationally. Modern Neural Network architectures such as Xception and MobileNet use a special type of Convolution called Depthwise Separable Convolution to speed up training and inference, especially on Mobile and Embedded Devices. . The Vanilla Convolution Operation . The convolution function can be mathematically defined as the following: . (f‚äõg)(t)=‚à´‚àí‚àû‚àûf(œÑ)g(t‚àíœÑ)dœÑ(f circledast g)(t) = int_{- infty}^{ infty} f( tau) g(t - tau) d tau(f‚äõg)(t)=‚à´‚àí‚àû‚àû‚Äãf(œÑ)g(t‚àíœÑ)dœÑ . For all non-negative values of t (i.e, for all values of t such that t ‚àà [0, ‚àû) ), we could truncate the limits of integration resulting in, . (f‚äõg)(t)=‚à´0tf(œÑ)g(t‚àíœÑ)dœÑ(f circledast g)(t) = int_{0}^{t} f( tau) g(t - tau) d tau(f‚äõg)(t)=‚à´0t‚Äãf(œÑ)g(t‚àíœÑ)dœÑ . It can also be defined as the overlap of two functions f and g as one slides over the other, performing a sum of products. . Source: https://en.wikipedia.org/wiki/Convolution Convolution as Sum of Products. Source: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1 Computational Complexity of Convolution . In order to decide the computational complexity of the convolutional operation, we would count the number of multiplication operations for a convolution. This is because the Binary Addition of two numbers may be performed in a single clock cycle using two registers with the inputs latched and a bunch of combinatorial logic gates. Binary multiplication, however, requires successive shift and addition operations which must be performed as many times as there are bits in the multiplier and is thus a more expensive operation. . Let us consider an input volume of the dimension (DVD_{V}DV‚Äã, DVD_{V}DV‚Äã, N) where DVD_{V}DV‚Äã is the height and width of the volume and NNN is the number of channels. In the case of a standard RGB image N=3N = 3N=3 and for a gray-scale image N=1N = 1N=1. . Let us convolve V with a tensor of shape (DVD_{V}DV‚Äã, DVD_{V}DV‚Äã, N) or N tensors with the shape (DkD_{k}Dk‚Äã, DkD_{k}Dk‚Äã) which results in a volume GGG of shape (DGD_{G}DG‚Äã, DGD_{G}DG‚Äã, N). . Let us count the number of multiplication operations for this operation. . Number of multiplication operations for a single stride across a single channel = Dk‚àóDkD_{k} * D_{k}Dk‚Äã‚àóDk‚Äã. . For M channels in the initial volume, the number of multiplication operations = (Dk)2‚àóM(D_{k})^{2} * M(Dk‚Äã)2‚àóM. . Sliding the kernel over a volume of (DVD_{V}DV‚Äã, DVD_{V}DV‚Äã, M), we get a tensor of shape (DGD_{G}DG‚Äã, DGD_{G}DG‚Äã, N). Hence the total number of multiplication operations for a single channel of the convolution kernel = (DG)2‚àó(Dk)2‚àóM(D_{G})^{2} * (D_{k})^{2} * M(DG‚Äã)2‚àó(Dk‚Äã)2‚àóM. . Since there are N channels in the convolutional kernel, this operation is repeated N times. Hence, the total number of multiplication operations for the above convolution operation = N‚àó(DG)2‚àó(Dk)2‚àóMN * (D_{G})^{2} * (D_{k})^{2} * MN‚àó(DG‚Äã)2‚àó(Dk‚Äã)2‚àóM. . Now, let us see how using an alternate form of the vanilla convolution operation, we can reduce time complexity. . Depthwise Separable Convolution . In the vanilla convolution operation all, the kernel is applied to all the channels of the input volume. However, Depthwise Separable Convolutions breaks down the whole operation into 2 steps: . Depthwise Convolution or the Filtering Stage | Pointwise Convolution or the Combination Stage | Depthwise Convolutions . Let us consider the same input volume (DVD_{V}DV‚Äã, DVD_{V}DV‚Äã, M) convolving with M (DKD_{K}DK‚Äã, DKD_{K}DK‚Äã) kernels. A single convolution with a single kernel gives a volume of (DGD_{G}DG‚Äã, DGD_{G}DG‚Äã, 1). Repeating this N times, we get N such tensors and stacking them up channel-wise, we get a single tensor of shape (DGD_{G}DG‚Äã, DGD_{G}DG‚Äã, M). . Now, the number of multiplication operations for a single kernel convolving over a single input channel = DK‚àóDKD_{K} * D_{K}DK‚Äã‚àóDK‚Äã. When the convolution is applied over an entire input volume . Let us now find the computational complexity for Depthwise Convolution. . The number of multiplication operations for the convolution of a single (DKD_{K}DK‚Äã, DKD_{K}DK‚Äã) kernel over a single stride over the input volume = (DK)2(D_{K})^{2}(DK‚Äã)2. . Since the output shape is (DGD_{G}DG‚Äã, DGD_{G}DG‚Äã), the number of multiplication operations for convolving over a single channel of the input image = (DG)2‚àó(DK)2(D_{G})^{2} * (D_{K})^{2}(DG‚Äã)2‚àó(DK‚Äã)2. . Since there are MMM number of kernels for convolving with MMM number of channels, the number of multiplication operations for Depthwise Convolution operation = M‚àó(DG)2‚àó(DK)2M * (D_{G})^{2} * (D_{K})^{2}M‚àó(DG‚Äã)2‚àó(DK‚Äã)2. . Pointwise Convolution . For Pointwise Convolution, we convolve the (DGD_{G}DG‚Äã, DGD_{G}DG‚Äã, M) volume with NNN kernels of (1, 1, MMM) producing the desired output of shape (DVD_{V}DV‚Äã, DVD_{V}DV‚Äã, N). . We will now find the computational complexity of the Pointwise Convolution operation. . For convolving a single kernel over a single stride of the input image, the number of multiplication operations = 1‚àó1‚àóM1 * 1 * M1‚àó1‚àóM = MMM. . For convolving a single kernel over a single channel of the input tensor producing a shape of (DGD_{G}DG‚Äã, DGD_{G}DG‚Äã), the number of multiplication operations = M‚àó(DG)2M * (D_{G})^{2}M‚àó(DG‚Äã)2. . For convolving NNN number of kernels over the whole of input tensor, the number of multiplication operations = N‚àóM‚àó(DG)2N * M * (D_{G})^{2}N‚àóM‚àó(DG‚Äã)2. . Comparing Vanilla Convolution with Depthwise Separable Convolution . Let us take the ratios between the Complexity of the Vanilla Convolution () operation and that of the Depthwise Separable Convolution operation. . convvanillaconvdsc=N‚àó(DG)2‚àó(DK)2‚àóMM‚àó(DG)2‚àó[(DK)2+N] frac{conv_{vanilla}}{conv_{dsc}} = frac{N * (D_{G})^{2} * (D_{K})^{2} * M}{M * (D_{G})^{2} * [(D_{K})^{2} + N]}convdsc‚Äãconvvanilla‚Äã‚Äã=M‚àó(DG‚Äã)2‚àó[(DK‚Äã)2+N]N‚àó(DG‚Äã)2‚àó(DK‚Äã)2‚àóM‚Äã . or, . convvanillaconvdsc=(DK)2‚àóM(DK)2+N frac{conv_{vanilla}}{conv_{dsc}} = frac{(D_{K})^{2} * M}{(D_{K})^{2} + N}convdsc‚Äãconvvanilla‚Äã‚Äã=(DK‚Äã)2+N(DK‚Äã)2‚àóM‚Äã . or, . convvanillaconvdsc=1(DK)2+1N frac{conv_{vanilla}}{conv_{dsc}} = frac{1}{(D_{K})^{2}} + frac{1}{N}convdsc‚Äãconvvanilla‚Äã‚Äã=(DK‚Äã)21‚Äã+N1‚Äã . Now, let us consider N=3N = 3N=3 and DK = [2 ** i for i in range(5, 11)] and visualize how the ratio varies. . Note that the ratio of Time Complexity of Vanilla Convolution to that of Depthwise Separable Convolution is always much less than 1 and it decreases with increasing Kernel Dimension, making it much faster compared to Vanilla Convolution. . Depthwise Separable Convolutions are widely used in building fast CNN architectures such as Xception, Mobilenet and Multi-modal Neural Networks. In the upcoming articles, we would discuss these two articles in detail. .",
            "url": "https://soumik12345.github.io/geekyrakshit/algebra/cnn/computervision/convolution/convolutionaneuralnetwork/datascience/deeplearning/depthwiseseperableconvolution/dnn/embeddeddevices/google/mathematics/maths/mobiledevices/mobilenet/multimodealnetwork/neural-networks/plotly/python/xception/2019/10/19/depthwise-seperable-convolution.html",
            "relUrl": "/algebra/cnn/computervision/convolution/convolutionaneuralnetwork/datascience/deeplearning/depthwiseseperableconvolution/dnn/embeddeddevices/google/mathematics/maths/mobiledevices/mobilenet/multimodealnetwork/neural-networks/plotly/python/xception/2019/10/19/depthwise-seperable-convolution.html",
            "date": " ‚Ä¢ Oct 19, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Nearest Celebrity Face using Deep Learning",
            "content": "Face Recognition . Face Recognition is one of the more interesting applications of Deep Learning i. At a single glance, it may seem like a simple classification problem; classify if this photo shows Soumik‚Äôs face or Souranil‚Äôs. You might be quick to jump to the conclusion, oh, the handsome face on the right belongs to Soumik and the ugly face on the left belongs to Souranil but, Ahem!!! We beg to differ. . Dear ConvNet, which of these is Soumik and which one is Souranil??? See, the problem with traditional ConvNets is that they need to look at lots of images of your face and lots of other faces to learn to correctly classify them; maybe a thousand images from each category. Imagine yourself as the teacher of a class where you want to install a system, which monitors every student and recognize them by faces so that they would not bunk classes (poor students üòì). In order to do so you have to collect 1000 mugshots of each of your students!!! Even if you manage to do this insane task, just imagine having to retrain the model again if a new student decides to join your lecture!!! . Ideally, we would want to verify the face of a person from any footage given only one photo of the person available in the database. Hence, our challenge, in this case, can be formalized as a One-Shot Learning problem. History has been witness to the fact that Deep Learning algorithms do not work well if you have only one training example. . One-Shot Learning . Let‚Äôs focus a bit more light on the problem discussed earlier with an example. Our dear friend Atul is supposed to appear for a video interview for a company and he wants to remain undisturbed during the interview, so he won‚Äôt allow anyone into his room other than Abhik who speaks a lot of Crox English and would help him with the interview (from behind the laptop of course). So, he wants sets up a system using his phone camera which would verify if the person who wants to enter his room is Abhik or not. Now, Atul can simply design a ConvNet with convolutional layers followed by a couple of fully connected layers ending in a softmax activation with 3 outputs corresponding to the three of us. . Which of us speaks the best Crox will be decided upon by Atul‚Äôs algorithm!!! There are several demerits to this approach such as . Atul does not have more than thirty of our mugshots which he painfully collected from Facebook and Instagram. Such a small training set would not be enough. | If he decides to accept help from Avishek besides Abhik, he would have to retrain the model again (after painfully collecting his mugshots) in order for the algorithm to recognize Avishek also. | . So, in order to make this work, Atul thinks of a different approach. Instead of building a model to differentiate various faces, he decides to build a model that would learn a Similarity Function D. Then that would say how similar the current image is with a mugshot of Abhik that is present in his dataset and he decides upon a similarity threshold œÑ upon meeting which the door will open. . D(image1, image2) = Degree of Difference between the Images D(image1, image2) &lt;= œÑ means images are same, and D(image1, image2) &gt; œÑ means images are different Siamese Networks . The job of the function D is to learn the level of difference between two different images of faces. A nice way to do this is by using Siamese Networks. While a traditional ConvNet consists of Convolutional layers followed by Fully Connected or Dense Layers which are then fed into a Softmax function to perform classification, in case of a Siamese Network, there is no Softmax unit, instead the last Dense layer acts as the output layers which gives a list or vector of numbers. Let us call the outputs for images x_1 and x_2, f(x_1) and f(x_2) respectively. Let‚Äôs say that the output layer has 128 fully connected units, hence, f(x_1) and f(x_2) will each be a vector of 128 numbers. f(x_1) and f(x_2) are called the Encoding of x_1 and x_2 respectively. . If we believe the encodings to be a good enough representation of the input images, we can define the function D as the square of norm of the difference between the Encodings. . D(x1,x2)=‚à£‚à£f(x1)‚àíf(x2)‚à£‚à£2D(x_{1}, x_{2}) = ||f(x_{1}) - f(x_{2})||^{2}D(x1‚Äã,x2‚Äã)=‚à£‚à£f(x1‚Äã)‚àíf(x2‚Äã)‚à£‚à£2 . Now the question comes that how do we train such a network. Since the same network is used to compute the Encodings from two different images, we have to train the parameters so that the trained network defines as accurate Encoding. . Training a Siamese Network . One way to learn the parameters of the neural network so that it gives you an accurate enough encoding for the images is to define an applied gradient descent on the Triplet Loss Function. In this case, we will have an anchor image, a positive image (the same person as the anchor image) and a negative image (a different person from the anchor image). Now we train the Siamese Network so that the distance between the anchor and the positive image is minimized. This also increases the distance between the anchor and the negative image is maximized. The fact that at each instance, we are looking at three images gives rise to the terminology Triplet Loss. The dataset, in this case, should consist of multiple triplets of Anchor, Positive and Negative Images. . Learning Objective . For Positive Image, . D(A,P)=‚à£‚à£f(A)‚àíf(P)‚à£‚à£2D(A, P) = ||f(A) - f(P)||^{2}D(A,P)=‚à£‚à£f(A)‚àíf(P)‚à£‚à£2 . For Negative Image, . D(A,N)=‚à£‚à£f(A)‚àíf(N)‚à£‚à£2D(A, N) = ||f(A) - f(N)||^{2}D(A,N)=‚à£‚à£f(A)‚àíf(N)‚à£‚à£2 . Learning Objective is . D(A,P)‚àíD(A,N)+Œ±‚â§0D(A, P) - D(A, N) + alpha leq 0D(A,P)‚àíD(A,N)+Œ±‚â§0 . where Œ± is defined as a margin and the Loss Function is given as . loss=‚àëi=1nmax(‚à£‚à£f(Ai)‚àíf(Pi)‚à£‚à£2‚àí‚à£‚à£f(Ai)‚àíf(Pi)‚à£‚à£2+Œ±,0)loss = sum_{i=1}^{n} max(||f(A_{i}) - f(P_{i})||^{2} - ||f(A_{i}) - f(P_{i})||^{2} + alpha, 0)loss=i=1‚àën‚Äãmax(‚à£‚à£f(Ai‚Äã)‚àíf(Pi‚Äã)‚à£‚à£2‚àí‚à£‚à£f(Ai‚Äã)‚àíf(Pi‚Äã)‚à£‚à£2+Œ±,0) . where n is the number of triplets in the dataset. . All the ideas till this point have been presented in the paper FaceNet by Florian Schroff, Dmitry Kalinichenko, and James Philbin. . After having trained the FaceNet on a large Triplet Dataset, we can use it to verify any face. Now, Atul would only need to store the Encodings of the faces of Abhik and Avishek. Then he would have to decide upon the value of Similarity Threshold œÑ. Whenever someone approaches the door, his face will be localized and passed through the FaceNet. If their D value is less than œÑ then it is a match!!! . Extending the Idea to Nearest Celebrity Face . I always wondered about those Facebook applications which used to predict stuff like which Celebrity or Footballer you look like. I tried building a similar application using the idea of FaceNet. Instead of keeping images of existing people to be verified in the database, I collected images of famous celebrities from Google Images, closely cropped their faces and stored their Encodings in the database. The Encodings were generated by a pre-trained FaceNet with an Inception backbone. Now theoretically, if I input some of my friends‚Äô faces, they should ideally match the closest image in the database. Well, I tried implementing it and the results were hilarious üòÇ . The project can be found at https://github.com/soumik12345/Nearest-Celebrity-Face. If you like the project and found the results to be hilarious, please leave a star on the Github repository. For more such exciting articles, stay tuned at http://geekyrakshit.com. .",
            "url": "https://soumik12345.github.io/geekyrakshit/computervision/deeplearning/facenet/inception/keras/nearestcelebrityface/python/tensorflow/2019/08/07/nearest-celebrity-face.html",
            "relUrl": "/computervision/deeplearning/facenet/inception/keras/nearestcelebrityface/python/tensorflow/2019/08/07/nearest-celebrity-face.html",
            "date": " ‚Ä¢ Aug 7, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://soumik12345.github.io/geekyrakshit/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://soumik12345.github.io/geekyrakshit/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}